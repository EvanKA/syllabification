{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CRF Syllable Experimentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgHFpW6jjLZLxSPoA42Vl2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EdIMWlMHMuw",
        "outputId": "1d9f56ca-bc65-4e87-b384-801a7419928f"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 11.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klMtlyIdH4HX",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "6204bf93-67ac-4309-a9fa-a36269b7062f"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "# Upload dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-782590ab-05d5-4702-87b7-2993a8d5da90\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-782590ab-05d5-4702-87b7-2993a8d5da90\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving preprocessed.txt to preprocessed.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36N2keisHwPH"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fWfKOcb1Cgu"
      },
      "source": [
        "def load_data():\n",
        "    # Loads in syllable data\n",
        "    dataframe = pd.read_csv(\"preprocessed.txt\",\n",
        "                            sep=\",\",\n",
        "                            encoding=\"ISO-8859-1\",\n",
        "                            names=[\"word\", \"label\"])\n",
        "    # Necessary to specify str type for pandas columns\n",
        "    dataframe = dataframe.astype(str)\n",
        "    words = dataframe['word'].tolist()\n",
        "    labels = dataframe['label'].tolist()\n",
        "    # Converts each label to numpy array\n",
        "    for i in range(0, len(labels)):\n",
        "        labels[i] = list(labels[i])\n",
        "        for j in range(0, len(labels[i])):\n",
        "            labels[i][j] = int(labels[i][j])\n",
        "    for i in range(0, len(labels)):\n",
        "        labels[i] = np.array(labels[i])\n",
        "\n",
        "    # Vectorises syllable strings by treating each character as a token\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "    tokenizer.fit_on_texts(words)\n",
        "    words = tokenizer.texts_to_sequences(words)\n",
        "    for i in range(0, len(words)):\n",
        "        words[i] = np.array(words[i], dtype=float)\n",
        "\n",
        "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        words, padding=\"post\", maxlen=15\n",
        "    )\n",
        "    padded_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        labels, padding=\"post\", maxlen=15\n",
        "    )\n",
        "\n",
        "    # Normalisation\n",
        "    maximum_token = 37\n",
        "    for element in range(0, len(words)):\n",
        "        words[element] = words[element] / maximum_token\n",
        "\n",
        "    # Shuffles data\n",
        "    seed = random.random()\n",
        "    random.seed(seed)\n",
        "    random.shuffle(padded_inputs)\n",
        "    random.seed(seed)\n",
        "    random.shuffle(padded_outputs)\n",
        "\n",
        "    # Splits into training, validation, and test sets (64-16-20 split)\n",
        "    training_inputs = padded_inputs[0:113590]\n",
        "    training_outputs = padded_outputs[0:113590]\n",
        "    validation_inputs = padded_inputs[113590:141987]\n",
        "    validation_outputs = padded_outputs[113590:141987]\n",
        "    test_inputs = padded_inputs[141987:]\n",
        "    test_outputs = padded_outputs[141987:]\n",
        "\n",
        "    return training_inputs, training_outputs, validation_inputs, validation_outputs, test_inputs, test_outputs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYy4CGNw0TT_"
      },
      "source": [
        "train_in, train_out, val_in, val_out, test_in, test_out = load_data()\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_in, train_out))\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((val_in, val_out))\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_BUFFER_SIZE = 500\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_dataset = validation_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDllu5AB1iky"
      },
      "source": [
        "def inception_module(inputs, units, residual=True):\n",
        "        # 1D version of Inception module, with residual connections.\n",
        "        inception_branch_1 = tf.keras.layers.Conv1D(units, kernel_size=1, strides=2, activation=\"tanh\")(inputs)\n",
        "        inception_branch_1 = tf.keras.layers.ZeroPadding1D(padding=(0, 15 - inception_branch_1.shape[1]))(inception_branch_1)\n",
        "\n",
        "        inception_branch_2 = tf.keras.layers.Conv1D(units, kernel_size=1, activation=\"tanh\")(inputs)\n",
        "        inception_branch_2 = tf.keras.layers.Conv1D(units, kernel_size=3, strides=2, activation=\"tanh\")(inception_branch_2)\n",
        "        inception_branch_2 = tf.keras.layers.ZeroPadding1D(padding=(0, 15 - inception_branch_2.shape[1]))(inception_branch_2)\n",
        "\n",
        "        inception_branch_3 = tf.keras.layers.AveragePooling1D(pool_size=3, strides=2)(inputs)\n",
        "        inception_branch_3 = tf.keras.layers.Conv1D(units, kernel_size=3, activation=\"tanh\")(inception_branch_3)\n",
        "        inception_branch_3 = tf.keras.layers.ZeroPadding1D(padding=(0, 15 - inception_branch_3.shape[1]))(inception_branch_3)\n",
        "\n",
        "        inception_branch_4 = tf.keras.layers.Conv1D(units, kernel_size=1, activation=\"tanh\")(inputs)\n",
        "        inception_branch_4 = tf.keras.layers.Conv1D(units, kernel_size=3, activation=\"tanh\")(inception_branch_4)\n",
        "        inception_branch_4 = tf.keras.layers.Conv1D(units, kernel_size=3, strides=2, activation=\"tanh\")(inception_branch_4)\n",
        "        inception_branch_4 = tf.keras.layers.ZeroPadding1D(padding=(0, 15 - inception_branch_4.shape[1]))(inception_branch_4)\n",
        "\n",
        "        if residual == True:\n",
        "            inception_output = tf.keras.layers.add([inception_branch_1, inception_branch_2, inception_branch_3, inception_branch_4])\n",
        "            inception_output = tf.keras.layers.concatenate([inception_output, inputs])\n",
        "            return inception_output\n",
        "        else:\n",
        "            inception_output = tf.keras.layers.add([inception_branch_1, inception_branch_2, inception_branch_3, inception_branch_4])\n",
        "            return inception_output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGgDxggK1lRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64618bb5-4892-4eeb-dd80-65fdb52eff82"
      },
      "source": [
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(15,))\n",
        "    embedded_inputs = tf.keras.layers.Embedding(64, 256, mask_zero=True)(inputs)\n",
        "\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True))(embedded_inputs)\n",
        "    x = tf.keras.layers.concatenate([x, embedded_inputs])\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.concatenate([x, embedded_inputs])\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True))(x)\n",
        "\n",
        "    inception_output = inception_module(embedded_inputs, 128, residual=False)\n",
        "\n",
        "    output = tf.keras.layers.concatenate([x, inception_output, embedded_inputs])\n",
        "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation=\"relu\"))(output)\n",
        "    decoded_sequence, potentials, sequence_length, kernel = tfa.layers.CRF(2)(output)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=inputs, outputs=[decoded_sequence, potentials, sequence_length, kernel]\n",
        "    )\n",
        "\n",
        "model = build_model()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4nN8h5H1_wK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920d7d53-e146-4385-a7d0-5eb6e973661b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 15)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 15, 256)      16384       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) (None, 15, 256)      296448      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 15, 128)      32896       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 15, 512)      0           bidirectional_6[0][0]            \n",
            "                                                                 embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, 15, 128)      32896       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling1d_2 (AveragePoo (None, 7, 256)       0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 13, 128)      49280       conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_7 (Bidirectional) (None, 15, 256)      493056      concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 8, 128)       32896       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, 7, 128)       49280       conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, 5, 128)       98432       average_pooling1d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 6, 128)       49280       conv1d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 15, 512)      0           bidirectional_7[0][0]            \n",
            "                                                                 embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_8 (ZeroPadding1D (None, 15, 128)      0           conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_9 (ZeroPadding1D (None, 15, 128)      0           conv1d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_10 (ZeroPadding1 (None, 15, 128)      0           conv1d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_11 (ZeroPadding1 (None, 15, 128)      0           conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_8 (Bidirectional) (None, 15, 256)      493056      concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 15, 128)      0           zero_padding1d_8[0][0]           \n",
            "                                                                 zero_padding1d_9[0][0]           \n",
            "                                                                 zero_padding1d_10[0][0]          \n",
            "                                                                 zero_padding1d_11[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 15, 640)      0           bidirectional_8[0][0]            \n",
            "                                                                 add_1[0][0]                      \n",
            "                                                                 embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 15, 256)      164096      concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "crf_2 (CRF)                     [(None, 15), (None,  522         time_distributed_2[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 1,808,522\n",
            "Trainable params: 1,808,522\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceqo8saM0h4g"
      },
      "source": [
        "@tf.function\n",
        "def crf_loss_func(potentials, sequence_length, kernel, y):\n",
        "    crf_likelihood, _ = tfa.text.crf_log_likelihood(\n",
        "        potentials, y, sequence_length, kernel\n",
        "    )\n",
        "    # likelihood to loss\n",
        "    flat_crf_loss = -1 * crf_likelihood\n",
        "    sample_weight = 4.108897148948174\n",
        "    flat_crf_loss = flat_crf_loss * sample_weight\n",
        "    crf_loss = tf.reduce_mean(flat_crf_loss)\n",
        "\n",
        "    return crf_loss\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(0.002)\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "validation_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
        "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        decoded_sequence, potentials, sequence_length, kernel = model(x)\n",
        "        crf_loss = crf_loss_func(potentials, sequence_length, kernel, y)\n",
        "        loss = crf_loss + tf.reduce_sum(model.losses)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    \n",
        "    train_acc_metric.update_state(y, decoded_sequence)\n",
        "    train_loss(loss)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSdjcuso0ik_",
        "outputId": "1cce0e1f-2036-4604-c316-080957204429"
      },
      "source": [
        "EPOCHS = 25\n",
        "for epoch in range(EPOCHS):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    validation_loss.reset_states()\n",
        "    train_acc_metric.reset_states()\n",
        "    val_acc_metric.reset_states()\n",
        "\n",
        "    for x, y in train_dataset:\n",
        "        train_step(x, y)\n",
        "\n",
        "    print(f\"E{epoch+1} loss: {train_loss.result():.4f}\")\n",
        "    print(f\"E{epoch+1} binary_accuracy: {train_acc_metric.result():.4f}\")\n",
        "\n",
        "    for x, y in validation_dataset:\n",
        "        decoded_sequence, potentials, sequence_length, kernel = model(x, training=False)\n",
        "        val_acc_metric.update_state(y, decoded_sequence)\n",
        "        crf_loss = crf_loss_func(potentials, sequence_length, kernel, y)\n",
        "        loss = crf_loss + tf.reduce_sum(model.losses)\n",
        "        validation_loss(loss)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_loss = validation_loss.result()\n",
        "    print(f\"E{epoch+1} val_binary_accuracy: {val_acc:.4f}\")\n",
        "    print(f\"E{epoch+1} val_loss: {val_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E1 loss: 4.7259\n",
            "E1 binary_accuracy: 0.9503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/text/crf.py:546: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
            "  \"CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E1 val_binary_accuracy: 0.9613\n",
            "E1 val_loss: 3.6763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SY_f0cndYOZ"
      },
      "source": [
        "model.save_weights('my_checkpoint')\n",
        "files.download('my_checkpoint')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APDSbUQ4I4C3"
      },
      "source": [
        "# To do: FIX\n",
        "def predict_token(word):\n",
        "    dataframe = pd.read_csv(\"./preprocessed.txt\",\n",
        "                              sep=\",\",\n",
        "                              encoding=\"ISO-8859-1\",\n",
        "                              names=[\"word\", \"label\"])\n",
        "    dataframe = dataframe.astype(str)\n",
        "    words = dataframe['word'].tolist()\n",
        "\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "    tokenizer.fit_on_texts(words)\n",
        "\n",
        "    word = tokenizer.texts_to_sequences(word)\n",
        "    return np.array(word)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}