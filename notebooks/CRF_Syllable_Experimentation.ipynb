{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CRF Syllable Experimentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMw60lOyVZPq4iHHZ1UYlqL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EdIMWlMHMuw"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klMtlyIdH4HX"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "# Upload dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36N2keisHwPH"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fWfKOcb1Cgu"
      },
      "source": [
        "def load_data():\n",
        "    # Loads in syllable data\n",
        "    dataframe = pd.read_csv(\"preprocessed.txt\",\n",
        "                            sep=\",\",\n",
        "                            encoding=\"ISO-8859-1\",\n",
        "                            names=[\"word\", \"label\"])\n",
        "    # Necessary to specify str type for pandas columns\n",
        "    dataframe = dataframe.astype(str)\n",
        "    words = dataframe['word'].tolist()\n",
        "    labels = dataframe['label'].tolist()\n",
        "    # Converts each label to numpy array\n",
        "    for i in range(0, len(labels)):\n",
        "        labels[i] = list(labels[i])\n",
        "        for j in range(0, len(labels[i])):\n",
        "            labels[i][j] = int(labels[i][j])\n",
        "    for i in range(0, len(labels)):\n",
        "        labels[i] = np.array(labels[i])\n",
        "\n",
        "    # Vectorises syllable strings by treating each character as a token\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "    tokenizer.fit_on_texts(words)\n",
        "    words = tokenizer.texts_to_sequences(words)\n",
        "    for i in range(0, len(words)):\n",
        "        words[i] = np.array(words[i], dtype=float)\n",
        "\n",
        "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        words, padding=\"post\", maxlen=15\n",
        "    )\n",
        "    padded_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        labels, padding=\"post\", maxlen=15\n",
        "    )\n",
        "\n",
        "    # Normalisation\n",
        "    maximum_token = 37\n",
        "    for element in range(0, len(words)):\n",
        "        words[element] = words[element] / maximum_token\n",
        "\n",
        "    # Shuffles data\n",
        "    seed = random.random()\n",
        "    random.seed(seed)\n",
        "    random.shuffle(padded_inputs)\n",
        "    random.seed(seed)\n",
        "    random.shuffle(padded_outputs)\n",
        "\n",
        "    # Splits into training, validation, and test sets (64-16-20 split)\n",
        "    training_inputs = padded_inputs[0:113590]\n",
        "    training_outputs = padded_outputs[0:113590]\n",
        "    validation_inputs = padded_inputs[113590:141987]\n",
        "    validation_outputs = padded_outputs[113590:141987]\n",
        "    test_inputs = padded_inputs[141987:]\n",
        "    test_outputs = padded_outputs[141987:]\n",
        "\n",
        "    return training_inputs, training_outputs, validation_inputs, validation_outputs, test_inputs, test_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYy4CGNw0TT_"
      },
      "source": [
        "train_in, train_out, val_in, val_out, test_in, test_out = load_data()\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_in, train_out))\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((val_in, val_out))\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_BUFFER_SIZE = 500\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_dataset = validation_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDllu5AB1iky"
      },
      "source": [
        "def inception_module(inputs, units, residual=True):\n",
        "        # 1D version of Inception module, with residual connections.\n",
        "        inception_branch_1 = tf.keras.layers.Conv1D(units, kernel_size=1, strides=2, activation=\"tanh\")(inputs)\n",
        "        inception_branch_1 = tf.keras.layers.ZeroPadding1D(padding=(0, 15 - inception_branch_1.shape[1]))(inception_branch_1)\n",
        "\n",
        "        inception_branch_2 = tf.keras.layers.Conv1D(units, kernel_size=1, activation=\"tanh\")(inputs)\n",
        "        inception_branch_2 = tf.keras.layers.Conv1D(units, kernel_size=3, strides=2, activation=\"tanh\")(inception_branch_2)\n",
        "        inception_branch_2 = tf.keras.layers.ZeroPadding1D(padding=(0, 15 - inception_branch_2.shape[1]))(inception_branch_2)\n",
        "\n",
        "        inception_branch_3 = tf.keras.layers.AveragePooling1D(pool_size=3, strides=2)(inputs)\n",
        "        inception_branch_3 = tf.keras.layers.Conv1D(units, kernel_size=3, activation=\"tanh\")(inception_branch_3)\n",
        "        inception_branch_3 = tf.keras.layers.ZeroPadding1D(padding=(0, 15 - inception_branch_3.shape[1]))(inception_branch_3)\n",
        "\n",
        "        inception_branch_4 = tf.keras.layers.Conv1D(units, kernel_size=1, activation=\"tanh\")(inputs)\n",
        "        inception_branch_4 = tf.keras.layers.Conv1D(units, kernel_size=3, activation=\"tanh\")(inception_branch_4)\n",
        "        inception_branch_4 = tf.keras.layers.Conv1D(units, kernel_size=3, strides=2, activation=\"tanh\")(inception_branch_4)\n",
        "        inception_branch_4 = tf.keras.layers.ZeroPadding1D(padding=(0, 15 - inception_branch_4.shape[1]))(inception_branch_4)\n",
        "\n",
        "        if residual == True:\n",
        "            inception_output = tf.keras.layers.add([inception_branch_1, inception_branch_2, inception_branch_3, inception_branch_4])\n",
        "            inception_output = tf.keras.layers.concatenate([inception_output, inputs])\n",
        "            return inception_output\n",
        "        else:\n",
        "            inception_output = tf.keras.layers.add([inception_branch_1, inception_branch_2, inception_branch_3, inception_branch_4])\n",
        "            return inception_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGgDxggK1lRY"
      },
      "source": [
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(15,))\n",
        "    embedded_inputs = tf.keras.layers.Embedding(64, 256, mask_zero=True)(inputs)\n",
        "\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True))(embedded_inputs)\n",
        "    x = tf.keras.layers.concatenate([x, embedded_inputs])\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.concatenate([x, embedded_inputs])\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True))(x)\n",
        "\n",
        "    inception_output = inception_module(embedded_inputs, 128, residual=False)\n",
        "    inception_output = tf.keras.layers.MaxPooling1D()(inception_output)\n",
        "    inception_output =  tf.keras.layers.ZeroPadding1D(padding=(0, 15 - inception_output.shape[1]))(inception_output)\n",
        "\n",
        "    output = tf.keras.layers.concatenate([x, inception_output, embedded_inputs])\n",
        "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation=\"relu\"))(output)\n",
        "    decoded_sequence, potentials, sequence_length, kernel = tfa.layers.CRF(2)(output)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=inputs, outputs=[decoded_sequence, potentials, sequence_length, kernel]\n",
        "    )\n",
        "\n",
        "model = build_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4nN8h5H1_wK"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceqo8saM0h4g"
      },
      "source": [
        "@tf.function\n",
        "def crf_loss_func(potentials, sequence_length, kernel, y):\n",
        "    crf_likelihood, _ = tfa.text.crf_log_likelihood(\n",
        "        potentials, y, sequence_length, kernel\n",
        "    )\n",
        "    # likelihood to loss\n",
        "    flat_crf_loss = -1 * crf_likelihood\n",
        "    sample_weight = 4.108897148948174\n",
        "    flat_crf_loss = flat_crf_loss * sample_weight\n",
        "    crf_loss = tf.reduce_mean(flat_crf_loss)\n",
        "\n",
        "    return crf_loss\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(0.002)\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "validation_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
        "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        decoded_sequence, potentials, sequence_length, kernel = model(x)\n",
        "        crf_loss = crf_loss_func(potentials, sequence_length, kernel, y)\n",
        "        loss = crf_loss + tf.reduce_sum(model.losses)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    \n",
        "    train_acc_metric.update_state(y, decoded_sequence)\n",
        "    train_loss(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSdjcuso0ik_"
      },
      "source": [
        "EPOCHS = 25\n",
        "for epoch in range(EPOCHS):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "    validation_loss.reset_states()\n",
        "    train_acc_metric.reset_states()\n",
        "    val_acc_metric.reset_states()\n",
        "\n",
        "    for x, y in train_dataset:\n",
        "        train_step(x, y)\n",
        "\n",
        "    print(f\"E{epoch+1} loss: {train_loss.result():.4f}\")\n",
        "    print(f\"E{epoch+1} binary_accuracy: {train_acc_metric.result():.4f}\")\n",
        "\n",
        "    for x, y in validation_dataset:\n",
        "        decoded_sequence, potentials, sequence_length, kernel = model(x, training=False)\n",
        "        val_acc_metric.update_state(y, decoded_sequence)\n",
        "        crf_loss = crf_loss_func(potentials, sequence_length, kernel, y)\n",
        "        loss = crf_loss + tf.reduce_sum(model.losses)\n",
        "        validation_loss(loss)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_loss = validation_loss.result()\n",
        "    print(f\"E{epoch+1} val_binary_accuracy: {val_acc:.4f}\")\n",
        "    print(f\"E{epoch+1} val_loss: {val_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}